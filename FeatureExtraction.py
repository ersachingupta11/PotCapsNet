# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aFYwed-wT4MZREfjkZ5C9mYMQ-bIIZrg
"""

class ConvLayer(nn.Module):

    def __init__(self, in_channels=3, out_channels=32):
        super(ConvLayer, self).__init__()

        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(16, out_channels, kernel_size=5, stride=1, padding=2)
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1)  #can remove this also as default dilation=1
        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=1, padding=2, dilation=2)
        self.conv5 = nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=1, padding=2, dilation=3)

        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x2 = self.pool(F.relu(self.conv2(x)))
        # print(x2.size())
        x3 = F.relu(self.conv3(x2))
        x3_resized = torch.nn.functional.interpolate(x3, size=(52, 52), mode='bilinear', align_corners=False)
        # print(x3.size())
        x4 = F.relu(self.conv4(x2))
        # print(x4.size())
        x5 = F.relu(self.conv5(x2))
        # print(x5.size())
        # x4_resized = torch.nn.functional.interpolate(x4, size=(56, 56), mode='bilinear', align_corners=False)
        # print(x4.size())
        x5_resized = torch.nn.functional.interpolate(x5, size=(52, 52), mode='bilinear', align_corners=False)
        # print(x5_resized.size())

# Concatenate tensors along dimension 1
        x_output = torch.cat((x3_resized, x4, x5_resized),dim=2)
        # print(x_output.size())

        return x_output

class BasicConv(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):
        super(BasicConv, self).__init__()
        self.out_channels = out_planes
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None
        self.relu = nn.ReLU() if relu else None

    def forward(self, x):
        x = self.conv(x)
        if self.bn is not None:
            x = self.bn(x)
        if self.relu is not None:
            x = self.relu(x)
        return x

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class ChannelShuffle(nn.Module):
    def __init__(self, groups):
        super(ChannelShuffle, self).__init__()
        self.groups = groups

    def forward(self, x):
        batchsize, num_channels, height, width = x.data.size()
        channels_per_group = num_channels // self.groups

        # reshape
        x = x.view(batchsize, self.groups, channels_per_group, height, width)

        # transpose
        x = torch.transpose(x, 1, 2).contiguous()

        # reshape back to original shape
        x = x.view(batchsize, -1, height, width)

        return x

class ChannelGate(nn.Module):
    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], num_groups=8):
        super(ChannelGate, self).__init__()
        self.gate_channels = gate_channels
        self.num_groups = 8
        self.shuffle = nn.Sequential(
            nn.Conv2d(gate_channels, gate_channels, kernel_size=3, groups=num_groups, bias=False),
            nn.BatchNorm2d(gate_channels),
            nn.ReLU(inplace=True),
            ChannelShuffle(groups=num_groups)
        )
        self.mlp = nn.Sequential(
            Flatten(),
            nn.Linear(gate_channels, gate_channels // reduction_ratio),
            nn.ReLU(),
            nn.Linear(gate_channels // reduction_ratio, gate_channels)
            )
        self.pool_types = pool_types
    def forward(self, x):
        channel_att_sum = None
        for pool_type in self.pool_types:
            if pool_type=='avg':
                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))
                channel_att_raw = self.mlp( avg_pool )
            elif pool_type=='max':
                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))
                channel_att_raw = self.mlp( max_pool )
            elif pool_type=='lp':
                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))
                channel_att_raw = self.mlp( lp_pool )
            elif pool_type=='lse':
                # LSE pool only
                lse_pool = logsumexp_2d(x)
                channel_att_raw = self.mlp( lse_pool )

            if channel_att_sum is None:
                channel_att_sum = channel_att_raw
            else:
                channel_att_sum = channel_att_sum + channel_att_raw

        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)
        return x * scale

def logsumexp_2d(tensor):
    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)
    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)
    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()
    return outputs

class ChannelPool(nn.Module):
    def forward(self, x):
        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )