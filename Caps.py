# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aFYwed-wT4MZREfjkZ5C9mYMQ-bIIZrg
"""

class PrimaryCaps(nn.Module):

    def __init__(self, num_capsules=8, in_channels=32, out_channels=16):
        super(PrimaryCaps, self).__init__()

        self.capsules = nn.ModuleList([
            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                      kernel_size=3, stride=1, padding=1)
            for _ in range(num_capsules)])

    def forward(self, x):
        batch_size = x.size(0)
        u = [capsule(x).reshape(batch_size, 16 * 156 * 52, 1) for capsule in self.capsules]
        u = torch.cat(u, dim=-1)
        u_squash = self.squash(u)
        return u_squash

    def squash(self, input_tensor):
        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)
        scale = squared_norm / (1 + squared_norm) # normalization coeff
        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)
        return output_tensor

def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):
    for iteration in range(routing_iterations):
        m = nn.Softmax(dim=2)
        c_ij = m(b_ij)

        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)

        v_j = squash(s_j)

        if iteration < routing_iterations - 1:
            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)

            b_ij = b_ij + a_ij

    return v_j

TRAIN_ON_GPU = torch.cuda.is_available()
if(TRAIN_ON_GPU):
    print('Training on GPU!')
else:
    print('Only CPU available')

class DigitCaps(nn.Module):

    def __init__(self, num_capsules=3, previous_layer_nodes=16*156*52,
                 in_channels=8, out_channels=16):
        super(DigitCaps, self).__init__()

        self.num_capsules = num_capsules
        self.previous_layer_nodes = previous_layer_nodes
        self.in_channels = in_channels

        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes,
                                          in_channels, out_channels))

    def forward(self, u):
        u = u[None, :, :, None, :]
        W = self.W[:, None, :, :, :]

        u_hat = torch.matmul(u, W)

        b_ij = torch.zeros(*u_hat.size())

        if TRAIN_ON_GPU:
            b_ij = b_ij.cuda()

        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)

        return v_j


    def squash(self, input_tensor):
        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)
        scale = squared_norm / (1 + squared_norm)
        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)
        return output_tensor

