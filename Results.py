# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aFYwed-wT4MZREfjkZ5C9mYMQ-bIIZrg
"""

def generate_confusion_matrix(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)

        _, _, predictions = model(images)
        _, pred_labels = predictions.max(dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(pred_labels.cpu().numpy())

    cm = confusion_matrix(y_true, y_pred)

    return cm

confusion_mat = generate_confusion_matrix(capsule_net, testloader)

def plot_confusion_matrix(cm, classes):
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     ha="center", va="center",
                     color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

classes = ['Class 0', 'Class 1', 'Class 2']

plot_confusion_matrix(confusion_mat, classes)

def calculate_precision(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)


        _, _, predictions = model(images)
        _, pred_labels = predictions.max(dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(pred_labels.cpu().numpy())


    precision_scores = precision_score(y_true, y_pred, average=None)

    return precision_scores


precision_scores = calculate_precision(capsule_net, testloader)


print("Precision - Class 0:", precision_scores[0])
print("Precision - Class 1:", precision_scores[1])
print("Precision - Class 2:", precision_scores[2])

def calculate_f1_scores(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)


        _, _, predictions = model(images)
        _, pred_labels = predictions.max(dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(pred_labels.cpu().numpy())


    f1_scores = f1_score(y_true, y_pred, average=None)

    return f1_scores


f1_scores = calculate_f1_scores(capsule_net, testloader)


print("F1 Score - Class 0:", f1_scores[0])
print("F1 Score - Class 1:", f1_scores[1])
print("F1 Score - Class 2:", f1_scores[2])

def calculate_recall(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)


        _, _, predictions = model(images)
        _, pred_labels = predictions.max(dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(pred_labels.cpu().numpy())


    recall_scores = recall_score(y_true, y_pred, average=None)

    return recall_scores


recall_scores = calculate_recall(capsule_net, testloader)


print("Recall - Class 0:", recall_scores[0])
print("Recall - Class 1:", recall_scores[1])
print("Recall - Class 2:", recall_scores[2])

def calculate_sensitivity_specificity(model, dataloader, num_classes):

    tp = np.zeros(num_classes)
    fn = np.zeros(num_classes)
    tn = np.zeros(num_classes)
    fp = np.zeros(num_classes)

    model.eval()

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)


            _, _, logits = model(images)


            _, predicted_labels = torch.max(logits, 1)


            for i in range(num_classes):
                tp[i] += torch.sum((predicted_labels == i) & (labels == i)).item()
                fn[i] += torch.sum((predicted_labels != i) & (labels == i)).item()
                tn[i] += torch.sum((predicted_labels != i) & (labels != i)).item()
                fp[i] += torch.sum((predicted_labels == i) & (labels != i)).item()


    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    return sensitivity, specificity




num_classes = 3


sensitivity, specificity = calculate_sensitivity_specificity(capsule_net, testloader, num_classes)


for i in range(num_classes):
    print("Class {}: Sensitivity = {:.4f}, Specificity = {:.4f}".format(i, sensitivity[i], specificity[i]))